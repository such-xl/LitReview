import subprocess
from pathlib import Path
from typing import List, Dict
from . import PDFParser, ParsedPaper
import re
import json

class MinerUParser(PDFParser):
    """ä½¿ç”¨ MinerU (GPUåŠ é€Ÿ) è§£æ PDF"""
    
    def __init__(self, use_gpu=True, output_dir="./data/processed", llm=None):
        """åˆå§‹åŒ–MinerUè§£æå™¨
        
        Args:
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            output_dir: è¾“å‡ºç›®å½•
            llm: LLMå®ä¾‹ï¼Œç”¨äºæå–å…ƒæ•°æ®
        """
        self.use_gpu = use_gpu
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.llm = llm
        self._check_gpu()
    
    def _check_gpu(self):
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        if self.use_gpu:
            try:
                import torch
                if torch.cuda.is_available():
                    print(f"âœ“ GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
                else:
                    print("âš  GPU ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU")
                    self.use_gpu = False
            except ImportError:
                print("âš  PyTorch æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ CPU")
                self.use_gpu = False
    
    def parse(self, pdf_path: str) -> ParsedPaper:
        """è§£æPDFæ–‡ä»¶"""
        pdf_path = Path(pdf_path)
        
        print(f"{'ğŸš€ GPU' if self.use_gpu else 'ğŸ¢ CPU'} åŠ é€Ÿè§£æ: {pdf_path.name}")
        
        # ä½¿ç”¨MinerUè§£æ
        cmd = ["mineru", "-p", str(pdf_path), "-o", str(self.output_dir)]
        if self.use_gpu:
            cmd.extend(["--device", "cuda"])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"âš  MinerUå¤±è´¥ï¼Œå›é€€åˆ°PyMuPDF")
            return self._parse_with_pymupdf(pdf_path)
        
        # è¯»å–Jsonå’ŒMarkdown
        try:
            json_text = self._read_json_output(pdf_path)
        except FileNotFoundError:
            print("âš  æœªæ‰¾åˆ°JSONæ–‡ä»¶ï¼Œè·³è¿‡LLMæå–")
            json_text = "{}"
        
        markdown_text = self._read_markdown_output(pdf_path)
        
        # ç”¨LLMæå–å…ƒæ•°æ®
        metadata = self._extract_metadata_with_llm(json_text, markdown_text)
        
        # è§£æç»“æ„åŒ–ä¿¡æ¯
        return ParsedPaper(
            title=metadata.get('title') or self._extract_title(markdown_text),
            authors=metadata.get('authors') or self._extract_authors(markdown_text),
            abstract=metadata.get('abstract') or self._extract_abstract(markdown_text),
            full_text=markdown_text,
            markdown_text=markdown_text,
            sections=self._extract_sections(markdown_text),
            tables=self._extract_tables(markdown_text),
            equations=self._extract_equations(markdown_text),
            references=self._extract_references(markdown_text)
        )
    
    def _parse_with_pymupdf(self, pdf_path: Path) -> ParsedPaper:
        """å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨PyMuPDF"""
        try:
            import fitz
        except ImportError:
            raise ImportError("è¯·å®‰è£…: pip install PyMuPDF")
        
        doc = fitz.open(pdf_path)
        full_text = ""
        for page in doc:
            full_text += page.get_text()
        doc.close()
        
        return ParsedPaper(
            title=self._extract_title(full_text),
            authors=self._extract_authors(full_text),
            abstract=self._extract_abstract(full_text),
            full_text=full_text,
            markdown_text=full_text,
            sections=self._extract_sections(full_text),
            tables=[],
            equations=self._extract_equations(full_text),
            references=self._extract_references(full_text)
        )
    
    def _read_markdown_output(self, pdf_path: Path) -> str:
        """è¯»å–ç”Ÿæˆçš„Markdownæ–‡ä»¶"""
        pdf_name = pdf_path.stem
        possible_paths = [
            self.output_dir / pdf_name / "auto" / f"{pdf_name}.md",
            self.output_dir / pdf_name / f"{pdf_name}.md",
            self.output_dir / f"{pdf_name}.md",
        ]
        
        for md_path in possible_paths:
            if md_path.exists():
                with open(md_path, 'r', encoding='utf-8') as f:
                    return f.read()
        
        # æœç´¢æ‰€æœ‰.mdæ–‡ä»¶
        for md_path in self.output_dir.rglob("*.md"):
            return md_path.read_text(encoding='utf-8')
        
        raise FileNotFoundError(f"æœªæ‰¾åˆ°Markdownæ–‡ä»¶: {pdf_name}")
    
    def _read_json_output(self, pdf_path: Path) -> str:
        """è¯»å–ç”Ÿæˆçš„JSONæ–‡ä»¶"""
        pdf_name = pdf_path.stem
        possible_paths = [
            self.output_dir / pdf_name / "auto" / f"{pdf_name}.json",
            self.output_dir / pdf_name / f"{pdf_name}.json",
            self.output_dir / f"{pdf_name}.json",
        ]

        for json_path in possible_paths:
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    return f.read()
        
        # æœç´¢æ‰€æœ‰.jsonæ–‡ä»¶
        for json_path in self.output_dir.rglob("*.json"):
            return json_path.read_text(encoding='utf-8')

        raise FileNotFoundError(f"æœªæ‰¾åˆ°JSONæ–‡ä»¶: {pdf_name}")
    
    def _extract_metadata_with_llm(self, json_text: str, markdown_text: str) -> Dict:
        """ä½¿ç”¨LLMä»JSONå’ŒMarkdownä¸­æå–å…ƒæ•°æ®"""
        if not self.llm:
            return {}
        
        # è·å–LLMç±»å‹
        llm_type = self.llm.__class__.__name__.replace('Model', '').lower()
        
        try:
            first_page = self._get_first_page_content(json_text, markdown_text)
            
            prompt = f"""ä»ä»¥ä¸‹è®ºæ–‡é¦–é¡µå†…å®¹ä¸­æå–å…ƒæ•°æ®ï¼š

{first_page}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›nullï¼‰ï¼š
- title: è®ºæ–‡æ ‡é¢˜
- authors: ä½œè€…åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
- affiliations: ä½œè€…å•ä½ï¼ˆæ•°ç»„ï¼‰
- abstract: æ‘˜è¦
- keywords: å…³é”®è¯ï¼ˆæ•°ç»„ï¼‰
- publisher: å‡ºç‰ˆå•†/ä¼šè®®åç§°
- year: å‘è¡¨å¹´ä»½"""
            
            schema = {
                "title": "string",
                "authors": ["string"],
                "affiliations": ["string"],
                "abstract": "string",
                "keywords": ["string"],
                "publisher": "string",
                "year": "string"
            }
            
            result = self.llm.generate_structured(prompt, schema)
            print("\n" + "="*60)
            print("âœ“ LLMæå–æˆåŠŸ")
            print("="*60)
            print(f"  æ ‡é¢˜: {result.get('title', 'N/A')}")
            print(f"  ä½œè€…: {', '.join(result.get('authors', [])) if result.get('authors') else 'N/A'}")
            print(f"  å¹´ä»½: {result.get('year', 'N/A')}")
            print(f"  å‡ºç‰ˆå•†: {result.get('publisher', 'N/A')}")
            print(f"  å…³é”®è¯: {', '.join(result.get('keywords', [])) if result.get('keywords') else 'N/A'}")
            print(f"  æ‘˜è¦é•¿åº¦: {len(result.get('abstract', ''))} å­—ç¬¦")
            print("="*60 + "\n")
            return result
        except Exception as e:
            error_msg = str(e)
            print("\n" + "="*60)
            print("âŒ LLMæå–å¤±è´¥")
            print("="*60)
            print(f"  LLMç±»å‹: {llm_type}")
            
            if "503" in error_msg:
                print("  é”™è¯¯ç±»å‹: æœåŠ¡ä¸å¯ç”¨ (503)")
                if llm_type == "ollama":
                    print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ (ollama serve)")
                elif llm_type == "gemini":
                    print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥API Keyæ˜¯å¦æœ‰æ•ˆï¼Œæˆ–ç½‘ç»œæ˜¯å¦å¯è®¿é—®Google API")
                else:
                    print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦è¿è¡Œ")
            elif "timeout" in error_msg.lower():
                print("  é”™è¯¯ç±»å‹: è¯·æ±‚è¶…æ—¶")
                print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢åŠ è¶…æ—¶æ—¶é—´")
            elif "Connection" in error_msg or "connect" in error_msg.lower():
                print("  é”™è¯¯ç±»å‹: è¿æ¥å¤±è´¥")
                if llm_type == "ollama":
                    print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥OllamaæœåŠ¡åœ°å€ (http://localhost:11434)")
                elif llm_type == "gemini":
                    print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿å¯ä»¥è®¿é—®Google API")
                else:
                    print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥LLMæœåŠ¡åœ°å€å’Œç«¯å£")
            elif "API" in error_msg or "key" in error_msg.lower():
                print("  é”™è¯¯ç±»å‹: APIå¯†é’¥é”™è¯¯")
                print("  è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®")
            else:
                print(f"  é”™è¯¯ä¿¡æ¯: {error_msg}")
            print("  å›é€€æ–¹æ¡ˆ: å°†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–")
            print("="*60 + "\n")
        
        return {}
    
    def _get_first_page_content(self, json_text: str, markdown_text: str) -> str:
        """è·å–ç¬¬ä¸€é¡µå†…å®¹"""
        try:
            data = json.loads(json_text)
            # å°è¯•ä»JSONä¸­è·å–ç¬¬ä¸€é¡µ
            if isinstance(data, list) and len(data) > 0:
                first_page_blocks = [b for b in data if b.get("page_idx") == 0 and b.get("type") == "text"]
                page_text = " ".join(b["text"] for b in first_page_blocks)
                return page_text
            elif isinstance(data, dict):
                pages = data.get('pages', [])
                if pages:
                    return json.dumps(pages[0], ensure_ascii=False, indent=2)
        except:
            pass
        
        # å›é€€åˆ°Markdownå‰1000å­—ç¬¦
        return markdown_text[:1000]
    def _extract_title(self, text: str) -> str:
        """æå–æ ‡é¢˜"""
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        return lines[0] if lines else "Unknown Title"
    
    def _extract_authors(self, text: str) -> List[str]:
        """æå–ä½œè€…"""
        lines = text.split('\n')
        for i, line in enumerate(lines[:30]):
            if 'author' in line.lower():
                author_line = lines[i+1] if i+1 < len(lines) else line
                return [a.strip() for a in re.split(r'[,;]', author_line) if a.strip()]
        return ["Unknown Author"]
    
    def _extract_abstract(self, text: str) -> str:
        """æå–æ‘˜è¦"""
        match = re.search(r'(?:Abstract|ABSTRACT)\s*\n(.*?)(?:\n\n|\n[A-Z])', text, re.DOTALL)
        return match.group(1).strip() if match else ""
    
    def _extract_sections(self, text: str) -> Dict[str, str]:
        """æå–ç« èŠ‚"""
        sections = {}
        keywords = ['introduction', 'method', 'result', 'discussion', 'conclusion']
        lines = text.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            if any(k in line.lower() for k in keywords) and len(line.strip()) < 100:
                if current_section:
                    sections[current_section] = '\n'.join(current_content).strip()
                current_section = line.strip()
                current_content = []
            elif current_section:
                current_content.append(line)
        
        if current_section:
            sections[current_section] = '\n'.join(current_content).strip()
        return sections
    
    def _extract_tables(self, text: str) -> List[Dict]:
        """æå–è¡¨æ ¼ï¼ˆä»Markdownï¼‰"""
        tables = []
        table_pattern = r'\|.*\|\n\|[-:| ]+\|\n(\|.*\|\n)+'
        for match in re.finditer(table_pattern, text):
            tables.append({"content": match.group(0)})
        return tables
    
    def _extract_equations(self, text: str) -> List[str]:
        """æå–å…¬å¼"""
        equations = []
        for line in text.split('\n'):
            if any(s in line for s in ['=', 'âˆ‘', 'âˆ«', 'âˆ‚', '$$']):
                if 10 < len(line.strip()) < 200:
                    equations.append(line.strip())
        return equations[:50]
    
    def _extract_references(self, text: str) -> List[str]:
        """æå–å‚è€ƒæ–‡çŒ®"""
        match = re.search(r'(?:References|REFERENCES)\s*\n(.*?)$', text, re.DOTALL)
        if match:
            ref_text = match.group(1)
            return [r.strip() for r in re.split(r'\n\[\d+\]|\n\d+\.', ref_text) if len(r.strip()) > 30]
        return []


def create_mineru_parser(use_gpu=True, llm_provider="ollama", llm_model="llama2"):
    """ä¾¿æ·å·¥å‚æ–¹æ³•ï¼šåˆ›å»ºå¸¦LLMçš„MinerUè§£æå™¨
    
    Args:
        use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
        llm_provider: LLMæä¾›å•† (ollama/openai/claude/custom)
        llm_model: æ¨¡å‹åç§°
    
    Returns:
        é…ç½®å¥½çš„MinerUParserå®ä¾‹
    """
    from src.llm import LLMFactory
    
    try:
        llm = LLMFactory.create_llm(provider=llm_provider, model=llm_model)
        print(f"âœ“ LLMå·²åŠ è½½: {llm_provider}/{llm_model}")
    except Exception as e:
        print(f"âš  LLMåŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼")
        llm = None
    
    return MinerUParser(use_gpu=use_gpu, llm=llm)


class MinerUChunker:
    """MinerUè§£æå™¨çš„åˆ†å—å’Œå­˜å‚¨å·¥å…·ï¼ˆç”¨äºå‘é‡æ•°æ®åº“ï¼‰"""
    
    def __init__(self, parser: MinerUParser = None):
        self.parser = parser or MinerUParser()
    
    def chunk_text(self, text: str, chunk_size=1000, overlap=200) -> List[str]:
        """æ–‡æœ¬åˆ†å—"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            if end < len(text):
                for sep in ['ã€‚\n', 'ã€‚', '\n\n', '\n', '. ', '.']:
                    last_sep = chunk.rfind(sep)
                    if last_sep > chunk_size * 0.5:
                        chunk = chunk[:last_sep + len(sep)]
                        end = start + last_sep + len(sep)
                        break
            
            if chunk.strip():
                chunks.append(chunk.strip())
            start = end - overlap
        
        return chunks


if __name__ == "__main__":
    from src.llm import LLMFactory
    
    # åˆ›å»ºLLMå®ä¾‹ï¼ˆå¯é€‰ï¼‰
    try:
        llm = LLMFactory.create_llm(provider="ollama", model="llama2")
        print("âœ“ LLMå·²åŠ è½½ï¼Œå°†ä½¿ç”¨æ™ºèƒ½å…ƒæ•°æ®æå–")
    except Exception as e:
        print(f"âš  LLMåŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–")
        llm = None
    
    # åˆ›å»ºè§£æå™¨
    parser = MinerUParser(use_gpu=True, llm=llm)
    
    pdf_file = "data/pdfs/a.pdf"
    if Path(pdf_file).exists():
        result = parser.parse(pdf_file)
        print(f"\næ ‡é¢˜: {result.title}")
        print(f"ä½œè€…: {', '.join(result.authors)}")
        print(f"æ‘˜è¦: {result.abstract[:200]}...")
        print(f"ç« èŠ‚æ•°: {len(result.sections)}")
    else:
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_file}")