import subprocess
from pathlib import Path
from typing import List, Dict
import re
import time


class MinerUParser:
    """ä½¿ç”¨ MinerU (Docker) è§£æ PDF"""
    
    def __init__(self, output_dir="./data/MinerU", backend="vlm-http-client", vlm_url="http://127.0.0.1:30000"):
        """åˆå§‹åŒ–MinerUè§£æå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            backend: VLMåç«¯ç±»å‹
            vlm_url: VLMæœåŠ¡URL
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.backend = backend
        self.vlm_url = vlm_url
    
    def parse(self, pdf_path: str, timeout=300) -> str:
        """è§£æPDFæ–‡ä»¶ï¼Œè¿”å›Markdownæ–‡æœ¬
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            str: è§£æåçš„Markdownæ–‡æœ¬
        """
        pdf_path = Path(pdf_path)
        print(f"ğŸš€ è§£æ: {pdf_path.name}")
        
        # ä½¿ç”¨MinerUè§£æ
        cmd = ["mineru", "-p", str(pdf_path), "-o", str(self.output_dir), "-b", self.backend, "-u", self.vlm_url]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
        
        if result.returncode != 0:
            print(f"stderr: {result.stderr}")
            print(f"stdout: {result.stdout}")
            raise RuntimeError(f"MinerUè§£æå¤±è´¥: {result.stderr}")
        
        print("âœ“ MinerUå‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œç­‰å¾…æ–‡ä»¶ç”Ÿæˆ...")
        
        # ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ
        return self._read_markdown_output(pdf_path, wait_time=300)
    
    def _read_markdown_output(self, pdf_path: Path, wait_time=30) -> str:
        """è¯»å–ç”Ÿæˆçš„Markdownæ–‡ä»¶ï¼Œæ”¯æŒç­‰å¾…"""
        pdf_name = pdf_path.stem
        possible_paths = [
            self.output_dir / pdf_name / "auto" / f"{pdf_name}.md",
            self.output_dir / pdf_name / f"{pdf_name}.md",
            self.output_dir / f"{pdf_name}.md",
        ]
        
        # è½®è¯¢ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ
        start_time = time.time()
        while time.time() - start_time < wait_time:
            for md_path in possible_paths:
                if md_path.exists():
                    print(f"âœ“ æ‰¾åˆ°æ–‡ä»¶: {md_path}")
                    return md_path.read_text(encoding='utf-8')
            
            # æœç´¢æ‰€æœ‰.mdæ–‡ä»¶
            for md_path in self.output_dir.rglob("*.md"):
                if pdf_name in md_path.stem:
                    print(f"âœ“ æ‰¾åˆ°æ–‡ä»¶: {md_path}")
                    return md_path.read_text(encoding='utf-8')
            
            time.sleep(1)
            print(f"ç­‰å¾…æ–‡ä»¶ç”Ÿæˆ... ({int(time.time() - start_time)}s)")
        
        # è¶…æ—¶åæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        print(f"\næŸ¥æ‰¾è·¯å¾„:")
        for p in possible_paths:
            print(f"  - {p} (å­˜åœ¨: {p.exists()})")
        print(f"\nè¾“å‡ºç›®å½•å†…å®¹:")
        for p in self.output_dir.rglob("*"):
            print(f"  - {p}")
        
        raise FileNotFoundError(f"æœªæ‰¾åˆ°Markdownæ–‡ä»¶: {pdf_name} (ç­‰å¾…{wait_time}ç§’åè¶…æ—¶)")


class MinerUChunker:
    """MinerUè§£æå™¨çš„åˆ†å—å·¥å…·ï¼ˆç”¨äºå‘é‡æ•°æ®åº“ï¼‰"""
    
    def __init__(self, parser: MinerUParser = None):
        self.parser = parser or MinerUParser()
    
    def parse_and_chunk(self, pdf_path: str, max_chunk_size=1500) -> List[Dict[str, str]]:
        """è§£æPDFå¹¶æŒ‰æ®µè½åˆ†å—
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            max_chunk_size: æœ€å¤§å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            
        Returns:
            List[Dict]: [{"text": "...", "index": 0}, ...]
        """
        # è§£æPDF
        markdown_text = self.parser.parse(pdf_path)
        
        # åˆ†å—
        chunks = self._chunk_by_paragraphs(markdown_text, max_chunk_size)
        
        # æ·»åŠ ç´¢å¼•
        return [{"text": chunk, "index": i} for i, chunk in enumerate(chunks)]
    
    def _chunk_by_paragraphs(self, text: str, max_size: int) -> List[str]:
        """æŒ‰æ®µè½åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´"""
        paragraphs = re.split(r'\n\n+', text)
        chunks = []
        current_chunk = []
        current_size = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            para_size = len(para)
            
            # å•ä¸ªæ®µè½è¶…é•¿ï¼Œå¼ºåˆ¶åˆ‡åˆ†
            if para_size > max_size:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_size = 0
                chunks.extend(self._force_split(para, max_size))
            # åŠ å…¥å½“å‰æ®µè½ä¼šè¶…é•¿ï¼Œå…ˆä¿å­˜å½“å‰å—
            elif current_size + para_size > max_size:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_size = para_size
            # æ­£å¸¸ç´¯åŠ 
            else:
                current_chunk.append(para)
                current_size += para_size
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def _force_split(self, text: str, max_size: int) -> List[str]:
        """å¼ºåˆ¶åˆ‡åˆ†è¶…é•¿æ®µè½ï¼ˆä¿ç•™å¥å­å®Œæ•´æ€§ï¼‰"""
        chunks = []
        sentences = re.split(r'([ã€‚.!?]\s*)', text)
        current = ""
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else "")
            if len(current) + len(sentence) > max_size:
                if current:
                    chunks.append(current.strip())
                current = sentence
            else:
                current += sentence
        
        if current:
            chunks.append(current.strip())
        
        return chunks


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    chunker = MinerUParser()
    
    pdf_file = "data/pdfs/unparsed/conference_101719.pdf"
    chunker.parse(pdf_path=pdf_file)
    # if Path(pdf_file).exists():
    #     chunks = chunker.parse_and_chunk(pdf_file, max_chunk_size=1500)
    #     print(f"\nâœ“ è§£æå®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
    #     print(f"\nç¬¬ä¸€å—é¢„è§ˆ:\n{chunks[0]['text'][:200]}...")
    # else:
    #     print(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_file}")
